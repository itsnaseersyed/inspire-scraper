# INSPIRE Manak - Sceince Labs and Schools Contact Details Scraper

A production-ready web application for scraping school contact details from the INSPIRE Awards website with an intuitive interface.

## ğŸš€ Quick Start

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Run the application
python app.py
```

Open browser at: **http://localhost:5000**

## ğŸ“‹ Features

âœ… **User-Friendly Web Interface** - No coding required  
âœ… **State & District Selection** - Cascading dropdown menus  
âœ… **Real-Time Progress Tracking** - See exactly what's being scraped  
âœ… **Organized Output** - Data saved as CSV files by state/district  
âœ… **Multi-District Support** - Scrape all districts or select specific ones  
âœ… **Production Ready** - Error handling, retry logic, and logging  

## ğŸ¯ How to Use

1. **Select a State** from the dropdown
2. **Select Districts** (auto-loads after state selection)
   - Choose "âœ“ All Districts" for entire state
   - Or Ctrl+Click specific districts
3. **Click "Start Scraping"**
4. **Monitor Progress** in real-time
5. **Find Data** in `output/StateName/DistrictName.csv`

## ğŸ“ Output Structure

```
output/
â”œâ”€â”€ Andhra_Pradesh/
â”‚   â”œâ”€â”€ Allurisitharamaraju.csv
â”‚   â”œâ”€â”€ Guntur.csv
â”‚   â”œâ”€â”€ Krishna.csv
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Telangana/
â”‚   â”œâ”€â”€ Hyderabad.csv
â”‚   â””â”€â”€ ...
â””â”€â”€ scraper.log
```

## ğŸ“Š Data Format

Each CSV contains:
- State
- District
- School
- Contact Name
- Mobile Number
- Email
- Application Number

## ğŸ› ï¸ Configuration

Edit `scraper_backend.py` â†’ `Config` class to modify:

```python
REQUEST_TIMEOUT = 30        # Request timeout (seconds)
MAX_RETRIES = 3             # Retry attempts
RATE_LIMIT_DELAY = 0.5      # Delay between requests (seconds)
```

## ğŸ“¦ Project Structure

```
SCRAP/
â”œâ”€â”€ app.py                  # Flask web server
â”œâ”€â”€ scraper_backend.py      # Scraper engine
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Web interface
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ .gitignore             # Git ignore rules
â””â”€â”€ output/                # Generated data
```

## ğŸŒ Deployment

### Option 1: Local Deployment

```bash
python app.py
```

Access at `http://localhost:5000`

### Option 2: Production Deployment (Gunicorn)

```bash
# Install gunicorn
pip install gunicorn

# Run with gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

### Option 3: Docker Deployment

Create `Dockerfile`:

```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 5000
CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]
```

Build and run:

```bash
docker build -t inspire-scraper .
docker run -p 5000:5000 -v $(pwd)/output:/app/output inspire-scraper
```

## âš™ï¸ Environment Variables (Optional)

Create `.env` file:

```env
FLASK_ENV=production
FLASK_DEBUG=False
PORT=5000
```

## ğŸ“ API Endpoints

- `GET /` - Web interface
- `GET /api/states` - Get list of states
- `GET /api/districts/<state_id>` - Get districts for state
- `POST /api/start` - Start scraping
- `GET /api/status` - Get scraping status
- `GET /api/download` - Download results

## ğŸ”’ Security Notes

For production deployment:
1. Set `debug=False` in `app.py`
2. Use a reverse proxy (nginx/Apache)
3. Enable HTTPS
4. Set up rate limiting
5. Configure firewall rules

## ğŸ› Troubleshooting

**Port already in use:**
```bash
# Change port in app.py
app.run(port=8080)
```

**Dependencies not installing:**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**No data scraped:**
- Check logs in `output/scraper.log`
- Verify internet connection
- Ensure website is accessible

## ğŸ“„ License

This project is for educational purposes only.

## ğŸ¤ Support

For issues or questions, check `output/scraper.log` for detailed error messages.

---

**Version:** 2.0  
**Last Updated:** December 2025
